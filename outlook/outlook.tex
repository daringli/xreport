As of today, 2015-06-02, the \codename{} code is complete in the sense that it has models for all the quantities needed in a statistical decay code.
However, there is a reason that the original, \prgname{Codex} code had something close to seven models for everything: the code would be more accurate if it was used together with models suited for specific nuclei.

At this point, it should be fairly straightforward to implement new models. The object oriented structure of the code means that the user just has to inherit the appropriate model object, and override the model they wish to change. To allow for the multiple models to coexist without forcing the user to recompile, it may be suitable to implement a \emph{factory method} to declare which model class to use, and a corresponding new command-line option to control this factor method.

Since most of the functions accept a \emph{Nucleus object} as input, the interface should have no problems to accomodate more general input parameters. For example: nuclei with a given deformation could be implemented by inheriting the nucleus class, and the same interface could be used for all these classes. 
There are some potential design problems with different models for deformation dependent parameters, since this places a burden on the models to implement functions for all kinds of deformation; the program may need to signal to the user when it cannot handle their nucleus subclass, so that it does not just run the undeformed case without the users knowledge. To some extent, it may be possible to place some of these burdens on the person implementing the nucleus subclasses, by forcing them to write functions to convert their deformation parametrization to a standardized one, but that may not be straightforward, especially if nuclei at the saddle-point for fission were to be implemented, with their rather distinct $x$-parameter shapes.

The current setup, with the simulation steps split into separate programs, should be very easy to run in parallel with something like \prgname{GNU parallel}, but passing large amounts of data through a pipe may not be optimal from a performance point of view. At one point, it seemed as if performance would not be an issue, which motivated the decision to split up the program, but it may be worth to consider rewriting it with higher performance in mind. Although this structure has the advantage of having a standardized flow control (all sub-programs parse command-line options in much the same way), the long command lines required to run a specific tasks means that the user will want to keep it in a script, and it that point, they might as well have created an input file. 

In short, there are a lot of things that could be done for the code. 
Whether the code will actually do something for its users remains to be seen, and the curious reader is adviced to read the relevant appendix on how to get started!